import numpy as np# reward learning model for concurrent 6 port task# variablesn_ports   = 6rrs       = [30, 60, 240, 240, 1200, 2400] # reward schedules in seconds max_steps = 10800                          # max no. timesteps in seconds policy    = 'random'# parameterspr = np.zeros(n_ports)                     # agent's estimated probability of reward at each port g = []                                     # current goal statess = list(range(0,n_ports+1))              # state space. 0 is when agent is not at any port cs = []                                    # current state s_hist = []                                # state historyn_step = []                                # timestep number def check_outcome():    'Determine whether check is rewarded'    def select_policy(policy):        if policy == 'random':                prob = [1/6 for port_prob in pr]            return(prob)def intercheck_interval():    # General construction of a reward learning model# Can think about a few key components of such a model in its most general form:# 1. An moment-to-moment state estimate of the probability of reward at all six ports. This is Pr(t) where Pr is a vector with as many entries as reward ports. # 2. A decision process that evaluates Pr(t) to render a current decision about which port is the current goal. So some process mapping Pr(t) -> G(t) where G is a scalar. # 3. A navigational process that takes the agent from its current location to G(t).# Given this general framework I think many different behaviors can be captured within a description of how Pr(t) evolves over time. For example, in increasing levels of sophistication…# Pr(t) = ⅙ for all ports is a random search policy.# Pr(t) ~ ⅙ + e-(t-t(rew)) is something like a win/stay policy where the probability of sticking at the port after a reward is high, but then decays to random.# Pr(t) = total rewards / total time for each port is a matching policy where the probability of targeting a given port is the naive rate estimate# Pr(t) = Hazard(t) (assuming an agent knows/correctly estimates the true hazard for each port) is an optimal strategy that matches estimates to the likelihood that a reward is available at a given port at time t.# Pr(t,x,y) - could imagine formulations in which Pr is not only the hazard but also scaled by the distance of the agent from the reward port. # If Pr(t) is total rewards / time + distance this gets very close to MVT because it is the current rate penalized by some scaled version of travel cost (distance to ports from current location).# ideal observer: noiseless, just know what the intervals are - but still have sampling rate of checking    # what to plot from this model? cumulative rewards collected over time. Can fit 6 dimensional curve          agent's probability of checking is matched to the total number of checks that a mouse does - can query whether the agent wants to check at some frequency, or some probabilistic model that generates checks - can use actual distribution of checkscan take histogram of all intercheck intervals, sample from histogram integrating hazard functions over time, cross some threshold to check a port compare 15s to 30s in terms of check rate - if they're different then it's dependent on the environment, if not then there's some intrinsic rate'use montecarlo method to resample distribution - run simulation a bunch of times from empircial samplescan split histogram into just rewarded versus just unrewarded (because after reward maybe on average they take more or less time to check another port) - conditioned distributionsone back rewardcan do a GLM (montee carlos)        